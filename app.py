import streamlit as st
from huggingface_hub import InferenceClient
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¤— HF Inference Chatbot",
    layout="wide"
)
st.title("ğŸ¤— HF Inference Chatbot")

# Hugging Face í† í° ì„¤ì •
hf_token = st.secrets["HF_TOKEN"]
client = InferenceClient(model="mistralai/Mistral-7B-Instruct-v0.2", token=hf_token)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        response_container = st.empty()
        
        try:
            # ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                *[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
            ]
            
            # InferenceClientë¡œ ìƒì„± ìš”ì²­
            response = client.chat_completion(
                messages=messages,
                max_tokens=500,
                temperature=0.7
            )
            
            # ì‘ë‹µì—ì„œ ë‚´ìš© ì¶”ì¶œ
            ai_response = response.choices[0].message.content
            response_container.write(ai_response)
            
            # AI ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            st.session_state.messages.append({"role": "assistant", "content": ai_response})
            
        except Exception as e:
            response_container.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
